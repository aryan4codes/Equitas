# FairSight: AI Safety & Observability Platform

A hybrid SDK and backend platform that enhances OpenAI API usage with real-time safety, bias, and compliance checks.

## ğŸ¯ Overview

FairSight provides:
- **Client SDK**: Drop-in replacement for OpenAI API with safety enhancements
- **Guardian Backend**: Microservices for toxicity, bias, and jailbreak detection
- **Real-time Dashboard**: Observability UI for metrics and incidents
- **Multi-tenant**: Enterprise-grade data isolation and RBAC

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your App       â”‚
â”‚  + FairSight SDKâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º OpenAI API
         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Guardian Backend
                          â”œâ”€â”€ Toxicity Detector
                          â”œâ”€â”€ Bias Checker
                          â”œâ”€â”€ Jailbreak Detector
                          â”œâ”€â”€ Explainability Engine
                          â””â”€â”€ Remediation Service
                          
                          â†“
                          
                     Database (Logs, Incidents, Metrics)
                     
                          â†“
                          
                     Dashboard UI
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
cd backend
uv init --python 3.11
uv venv
source .venv/bin/activate
uv pip install -e .
```

### 2. Configure Environment

Create `.env` file:

```bash
# OpenAI
OPENAI_API_KEY=sk-your-key-here

# Database
DATABASE_URL=sqlite+aiosqlite:///./fairsight.db

# Security
SECRET_KEY=your-secret-key-change-in-production
```

### 3. Start Guardian Backend

```bash
cd backend
python -m guardian.main
```

Backend will be available at `http://localhost:8000`

### 4. Use FairSight SDK

```python
from fairsight_sdk import FairSight, SafetyConfig

# Initialize client
fairsight = FairSight(
    openai_api_key="sk-...",
    fairsight_api_key="fs-dev-key-123",
    tenant_id="your-org",
)

# Make safe API calls
response = fairsight.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello!"}],
    safety_config=SafetyConfig(on_flag="auto-correct")
)

# Access safety metadata
print(f"Toxicity: {response.safety_scores.toxicity_score}")
print(f"Categories: {response.safety_scores.toxicity_categories}")
```

## ğŸ“¦ Project Structure

```
backend/
â”œâ”€â”€ fairsight_sdk/          # Client SDK
â”‚   â”œâ”€â”€ client.py           # Main SDK client
â”‚   â”œâ”€â”€ models.py           # Data models
â”‚   â””â”€â”€ exceptions.py       # Custom exceptions
â”‚
â”œâ”€â”€ guardian/               # Backend API
â”‚   â”œâ”€â”€ main.py            # FastAPI app
â”‚   â”œâ”€â”€ core/              # Core utilities
â”‚   â”‚   â”œâ”€â”€ config.py      # Configuration
â”‚   â”‚   â”œâ”€â”€ database.py    # Database setup
â”‚   â”‚   â””â”€â”€ auth.py        # Authentication
â”‚   â”œâ”€â”€ models/            # Database models
â”‚   â”‚   â”œâ”€â”€ database.py    # SQLAlchemy models
â”‚   â”‚   â””â”€â”€ schemas.py     # Pydantic schemas
â”‚   â”œâ”€â”€ services/          # Analysis services
â”‚   â”‚   â”œâ”€â”€ toxicity.py    # Toxicity detection
â”‚   â”‚   â”œâ”€â”€ bias.py        # Bias checking
â”‚   â”‚   â”œâ”€â”€ jailbreak.py   # Jailbreak detection
â”‚   â”‚   â”œâ”€â”€ explainability.py  # Explanations
â”‚   â”‚   â””â”€â”€ remediation.py     # Content remediation
â”‚   â””â”€â”€ api/v1/            # API endpoints
â”‚       â”œâ”€â”€ analysis.py    # Analysis endpoints
â”‚       â”œâ”€â”€ logging.py     # Logging endpoint
â”‚       â”œâ”€â”€ metrics.py     # Metrics endpoint
â”‚       â””â”€â”€ incidents.py   # Incidents endpoint
â”‚
â””â”€â”€ examples/              # Usage examples
    â”œâ”€â”€ basic_usage.py     # SDK examples
    â””â”€â”€ test_guardian_api.py  # API testing
```

## ğŸ”’ Safety Features

### Toxicity Detection
- Uses OpenAI Moderation API
- Detects hate, harassment, violence, self-harm, sexual content
- Returns toxicity score (0-1) and flagged categories

### Bias Detection
- Demographic bias checking
- Paired prompt testing
- Stereotype detection

### Jailbreak Detection
- Pattern-based prompt injection detection
- Instruction override attempts
- Code injection prevention

### Explainability
- Highlights problematic text spans
- Natural language explanations
- Detailed violation categorization

### Automatic Remediation
- LLM-based text rewriting
- Removes toxic language while preserving intent
- Neutralizes biased content

## ğŸ“Š API Endpoints

### Analysis Endpoints

#### POST `/v1/analysis/toxicity`
Analyze text for toxicity.

```json
{
  "text": "Text to analyze",
  "tenant_id": "org123"
}
```

#### POST `/v1/analysis/bias`
Check for demographic bias.

```json
{
  "prompt": "Original prompt",
  "response": "LLM response",
  "tenant_id": "org123"
}
```

#### POST `/v1/analysis/jailbreak`
Detect jailbreak attempts.

```json
{
  "text": "Text to check",
  "tenant_id": "org123"
}
```

#### POST `/v1/analysis/explain`
Get explanation for flagged content.

```json
{
  "text": "Flagged text",
  "issues": ["toxicity", "bias"],
  "tenant_id": "org123"
}
```

#### POST `/v1/analysis/remediate`
Remediate unsafe content.

```json
{
  "text": "Unsafe text",
  "issue": "toxicity",
  "tenant_id": "org123"
}
```

### Logging & Metrics

#### POST `/v1/log`
Log API call with safety analysis.

#### GET `/v1/metrics`
Get aggregated metrics (usage, safety scores, incidents).

#### GET `/v1/incidents`
Query flagged incidents with filters.

## ğŸ”‘ Authentication

All endpoints require:
- **Authorization Header**: `Bearer <api-key>`
- **X-Tenant-ID Header**: `<tenant-id>`

Default API keys (for development):
- `fs-dev-key-123` â†’ `tenant_demo`
- `fs-prod-key-456` â†’ `tenant_prod`

## ğŸ“ˆ Metrics & Observability

FairSight logs comprehensive metrics per API call:

- **Safety Scores**: Toxicity, bias, jailbreak flags
- **Performance**: Latency, overhead, token counts
- **Usage**: Safety Inference Units (SIUs) consumed
- **Incidents**: Flagged content with severity levels

All data is isolated per tenant with encryption at rest.

## ğŸ›ï¸ Configuration

### Safety Config (SDK)

```python
SafetyConfig(
    on_flag="auto-correct",  # strict | auto-correct | warn-only
    toxicity_threshold=0.7,
    enable_bias_check=True,
    enable_jailbreak_check=True,
    enable_remediation=True,
)
```

### Tenant Config (Backend)

Stored in database per tenant:
- Safety thresholds
- Feature flags (enable/disable checks)
- Privacy settings (anonymization, retention)
- Credit limits (Safety Units)

## ğŸ§ª Testing

Run example scripts:

```bash
# Test SDK
python examples/basic_usage.py

# Test API directly
python examples/test_guardian_api.py
```

## ğŸ“ Development

### Running locally

```bash
# Start backend
uvicorn guardian.main:app --reload --port 8000

# In another terminal, test SDK
python examples/basic_usage.py
```

### Database migrations

```bash
# Auto-generate migration
alembic revision --autogenerate -m "Description"

# Apply migration
alembic upgrade head
```

## ğŸš¢ Deployment

### Docker

```bash
# Build
docker build -t fairsight-guardian .

# Run
docker run -p 8000:8000 --env-file .env fairsight-guardian
```

### Kubernetes

```bash
kubectl apply -f k8s/deployment.yaml
```

## ğŸ“œ License

MIT License - see LICENSE file

## ğŸ¤ Contributing

Contributions welcome! Please see CONTRIBUTING.md

## ğŸ“š Documentation

For detailed documentation, see:
- [PRD.md](PRD.md) - Product Requirements Document
- [API Documentation](http://localhost:8000/docs) - Swagger UI (when running)

## ğŸ†˜ Support

For issues or questions:
- GitHub Issues: [github.com/yourorg/fairsight/issues]
- Email: support@fairsight.ai

---

Built with â¤ï¸ for AI Safety
